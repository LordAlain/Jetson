{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1000 new images generated\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\ML Container\\TrafficSign\\data_aug.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.p'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# import cv2 as cv\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "from utils import next_batch, calculate_accuracy\n",
    "from model import Model\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    # Make all image array values fall within the range -1 to 1\n",
    "    # Note all values in original images are between 0 and 255, as uint8\n",
    "    X = X.astype('float32')\n",
    "#     X = (X - 128.) #/ 128.\n",
    "\n",
    "    # Convert the labels from numerical labels to one-hot encoded labels\n",
    "    y_onehot = np.zeros((y.shape[0], NUM_CLASSES))\n",
    "    for i, onehot_label in enumerate(y_onehot):\n",
    "        onehot_label[y[i]] = 1.\n",
    "    y = y_onehot\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Settings/parameters to be used later\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 32  # square image of size IMG_SIZE x IMG_SIZE\n",
    "GRAYSCALE = False  # convert image to grayscale?\n",
    "NUM_CHANNELS = 1 if GRAYSCALE else 3\n",
    "NUM_CLASSES = 43\n",
    "\n",
    "# Model parameters\n",
    "LR = 1e-3  # learning rate\n",
    "KEEP_PROB = 0.5  # dropout keep probability\n",
    "# OPT = tf.train.GradientDescentOptimizer(learning_rate=LR)  # choose which optimizer to use\n",
    "OPT = tf.optimizers.SGD(learning_rate=LR, momentum = 0, nesterov = False, name='SGD')\n",
    "\n",
    "# Training process\n",
    "RESTORE = False  # restore previous model, don't train?\n",
    "RESUME = False  # resume training from previously trained model?\n",
    "\n",
    "NUM_EPOCH = 100\n",
    "coef = 10\n",
    "\n",
    "BATCH_SIZE = 128  # batch size for training (relatively small)\n",
    "BATCH_SIZE_INF = 2048  # batch size for running inference, e.g. calculating accuracy\n",
    "VALIDATION_SIZE = 0.2  # fraction of total training set to use as validation set\n",
    "SAVE_MODEL = True  # save trained model to disk?\n",
    "MODEL_SAVE_PATH = './model.ckpt'  # where to save trained model\n",
    "\n",
    "%run -i data_aug.py\n",
    "\n",
    "# Load pickled data\n",
    "with open('train_aug.p', mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_train, y_train = preprocess_data(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "with open('test.p', mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "\n",
    "#Train/validation split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=VALIDATION_SIZE)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "x, y, logits, predictions, accuracy = model.x, model.y, model.logits, model.predictions, model.accuracy\n",
    "keep_prob = model.keep_prob\n",
    "loss = model.loss\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "grad_loss = coef * tf.reduce_sum(tf.nn.l2_loss(model.cw_grad))\n",
    "\n",
    "total_loss = model.loss + grad_loss\n",
    "\n",
    "# OPT = tf.train.GradientDescentOptimizer(learning_rate=LR) \n",
    "OPT = tf.optimizers.SGD(learning_rate=LR, momentum = 0, nesterov = False, name='SGD')\n",
    "\n",
    "optimizer = OPT.minimize(total_loss, global_step=global_step)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "if True:\n",
    "\n",
    "\tsaver = tf.train.Saver()\n",
    "\tfilename = tf.train.latest_checkpoint(\"./255_reg_model/\")\n",
    "\tprint(\"Latest training checkpoint is \", filename)\n",
    "\tif filename != None:\n",
    "\t\tsaver.restore(sess, filename)\n",
    "\telse:\n",
    "\t\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\tlast_time = time.time()\n",
    "\ttrain_start_time = time.time()\n",
    "\taccuracy_history = []\n",
    "\n",
    "\tfor epoch in range(NUM_EPOCH):\n",
    "\t\t# Instantiate generator for training data\n",
    "\t\ttrain_gen = next_batch(X_train, y_train, BATCH_SIZE, True)\n",
    "\n",
    "\t\t# How many batches to run per epoch\n",
    "\t\tnum_batches_train = math.ceil(X_train.shape[0] / BATCH_SIZE)\n",
    "\n",
    "\t\t# Run training on each batch\n",
    "\t\tfor _ in range(num_batches_train):\n",
    "\t\t\t# Obtain the training data and labels from generator\n",
    "\t\t\timages, labels = next(train_gen)\n",
    "\n",
    "\t\t\t# Perform gradient update (i.e. training step) on current batch\n",
    "\t\t\tsess.run(optimizer, feed_dict={x: images, y: labels, keep_prob: KEEP_PROB})\n",
    "\t\t# Calculate training and validation accuracy across the *entire* train/validation set\n",
    "\t\t# If train/validation size % batch size != 0\n",
    "\t\t# then we must calculate weighted average of the accuracy of the final (partial) batch,\n",
    "\t\t# w.r.t. the rest of the full batches\n",
    "\n",
    "\t\t# Training set\n",
    "\t\ttrain_gen = next_batch(X_train, y_train, BATCH_SIZE_INF, True)\n",
    "\t\ttrain_size = X_train.shape[0]\n",
    "\t\ttrain_acc = calculate_accuracy(train_gen, train_size, BATCH_SIZE_INF, accuracy, x, y, keep_prob, sess)\n",
    "\n",
    "\t\t# Validation set\n",
    "\t\tvalid_gen = next_batch(X_valid, y_valid, BATCH_SIZE_INF, True)\n",
    "\t\tvalid_size = X_valid.shape[0]\n",
    "\t\tvalid_acc = calculate_accuracy(valid_gen, valid_size, BATCH_SIZE_INF, accuracy, x, y, keep_prob, sess)\n",
    "        \n",
    "\n",
    "\t\ttest_gen = next_batch(X_test, y_test, BATCH_SIZE_INF, False)\n",
    "\t\ttest_size = X_test.shape[0]\n",
    "\t\ttest_acc = calculate_accuracy(test_gen, test_size, BATCH_SIZE_INF, accuracy, x, y, keep_prob, sess)\n",
    "\n",
    "\n",
    "\t\t# Record and report train/validation/test accuracies for this epoch\n",
    "\t\taccuracy_history.append((train_acc, valid_acc))\n",
    "\n",
    "\t\tprint('Epoch %d -- Train acc.: %.4f, Validation acc.: %.4f, Test acc.: %.4f, Elapsed time: %.2f sec' %\\\n",
    "\t\t    (epoch+1, train_acc, valid_acc, test_acc, time.time() - last_time))\n",
    "\t\tlast_time = time.time()\n",
    "\n",
    "\t\tif epoch % 10 == 9:\n",
    "\t\t#     # Also save accuracy history\n",
    "\t\t#     print('Accuracy history saved at accuracy_history.p')\n",
    "\t\t#     with open('accuracy_history.p', 'wb') as f:\n",
    "\t\t#         pickle.dump(accuracy_history, f)\n",
    "\t\t    saver.save(sess, os.path.join(\"./255_reg_model\", 'checkpoint'), global_step=global_step)\n",
    "\t\t    print('Model Saved !!!', epoch, \"\\n\")\n",
    "\n",
    "\ttotal_time = time.time() - train_start_time\n",
    "\tprint('Total elapsed time: %.2f sec (%.2f min)' % (total_time, total_time/60))\n",
    "\n",
    "\t# After training is complete, evaluate accuracy on test set\n",
    "\tprint('Calculating test accuracy...')\n",
    "\ttest_gen = next_batch(X_test, y_test, BATCH_SIZE_INF, False)\n",
    "\ttest_size = X_test.shape[0]\n",
    "\ttest_acc = calculate_accuracy(test_gen, test_size, BATCH_SIZE_INF, accuracy, x, y, keep_prob, sess)\n",
    "\tprint('Test acc.: %.4f' % (test_acc,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = next_batch(X_train, y_train, BATCH_SIZE, True)\n",
    "images, labels = next(train_gen)\n",
    "gloss = sess.run(model.grad_loss, feed_dict={x: images, y: labels, keep_prob: KEEP_PROB})\n",
    "\n",
    "print(gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "filename = tf.train.latest_checkpoint(\"./natural_model/\")\n",
    "print(\"Latest training checkpoint is \", filename)\n",
    "if filename != None:\n",
    "    saver.restore(sess, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = next_batch(X_train, y_train, BATCH_SIZE, True)\n",
    "images, labels = next(train_gen)\n",
    "gloss = sess.run(model.grad_loss, feed_dict={x: images, y: labels, keep_prob: KEEP_PROB})\n",
    "\n",
    "print(gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
