{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Train Keras version:  2.7.0\n",
      "Reg Train TF version:  2.7.0\n",
      "Tmpdir =  C:\\Users\\akomm\\AppData\\Local\\Temp\\tmp30k2qag3\n"
     ]
    }
   ],
   "source": [
    "# !pip3 uninstall PIL\n",
    "# !pip3 install --upgrade pip tensorflow\n",
    "# !pip3 install --upgrade --force-reinstall matplotlib pillow\n",
    "\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "# import cupy as cp\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "# import PIL\n",
    "# from pillow import Image\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import Input\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import *\n",
    "import tempfile\n",
    "\n",
    "print(\"Reg Train Keras version: \", keras.__version__)\n",
    "print(\"Reg Train TF version: \", tf.__version__)\n",
    "\n",
    "from ml_library.model import *\n",
    "from ml_library.utils import *\n",
    "from ml_library.config import *\n",
    "from ml_library.w_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_dir = './models/'\n",
    "#tmpdir = tempfile.mkdtemp()\n",
    "#print(\"Tmpdir = \", tmpdir)\n",
    "print(\"model_dir = \", model_dir)\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file:  True\n",
      "Test file:  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "importDatasets()\n",
    "\n",
    "\n",
    "# Load data\n",
    "training_file = \"./Datasets/GTSRB_Final_Training_Images.zip\"\n",
    "# testing_file = \"./Datasets/GTSRB_Final_Test_Images.zip\"\n",
    "\n",
    "train = generateTensor(training_file)\n",
    "# test = generateTensor(testing_file)\n",
    "\n",
    "x_train, y_train = split(train)\n",
    "# x_test, y_test = split(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 31367 Validation size: 7842 (0.200)\n",
      "No classes: 43\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (31367, 32, 32, 3)\n",
      "y_train shape: (31367,)\n",
      "x_val shape: (7842, 32, 32, 3)\n",
      "y_val shape: (7842,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "label_map = map_labels('signnames.csv')\n",
    "\n",
    "file_paths = len(x_train)\n",
    "from ml_library.model import *\n",
    "#Train/validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    np.asarray(x_train), np.asarray(y_train), test_size=VALIDATION_SIZE)\n",
    "\n",
    "\n",
    "valid_ratio = len(x_val) / file_paths\n",
    "print(\"Train size: {} Validation size: {} ({:0.3f})\".format(\n",
    "    len(x_train),\n",
    "    len(x_val),\n",
    "    valid_ratio))\n",
    "\n",
    "classes, dist = np.unique(y_train, return_counts=True)\n",
    "NUM_CLASSES = len(classes)\n",
    "print(\"No classes: {}\".format(NUM_CLASSES))\n",
    "\n",
    "plt.bar(classes, dist, align='center', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "# print(\"x_test shape:\", x_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from ml_library.model import *\n",
    "from ml_library.config import *\n",
    "\n",
    "# original_dim = 784\n",
    "original_dim = IMG_SIZE*IMG_SIZE\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "# Define our metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('val_accuracy')\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "# # Now we get a test dataset.\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "# test_dataset = test_dataset.batch(64)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(\n",
    "    1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "vae_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "vae_ds = vae_ds.shuffle(buffer_size=1024).batch(64)\n",
    "# vae_ds = vae_ds.cache().shuffle(\n",
    "#     1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "vae.compile(optimizer, loss= mse_loss_fn, metrics= loss_metric)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "# !rm - rf ./logs/fit/vae\n",
    "\n",
    "log_dir = \"logs/fit/vae\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# vae.fit(x=x_train, y=x_train, epochs=NUM_EPOCH, validation_data=(x_val, x_val), callbacks=[tensorboard_callback], batch_size=64, verbose=1)\n",
    "\n",
    "\n",
    "# epochs = NUM_EPOCH\n",
    "\n",
    "# # Iterate over epochs.\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     for step, x_batch_train in enumerate(vae_ds):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             reconstructed = vae(x_batch_train)\n",
    "#             # Compute reconstruction loss\n",
    "#             loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "#             loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "#         grads = tape.gradient(loss, vae.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "#         loss_metric(loss)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))\n",
    "\n",
    "\n",
    "# vae.evaluate(x_val, x_val, verbose=1)\n",
    "# model_path = os.path.join(model_dir, 'normal_vae')\n",
    "# print('Saving model...')\n",
    "# vae.save(model_path)\n",
    "# # tf.saved_model.save(vae, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 16, 16, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 6, 6, 64)          9280      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 3, 3, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 3, 3, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1024)              132096    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 43)                44075     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,456,939\n",
      "Trainable params: 1,456,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "491/491 [==============================] - 220s 445ms/step - loss: 2.1923 - accuracy: 0.3427 - val_loss: 1.1729 - val_accuracy: 0.6047\n",
      "Epoch 2/5\n",
      "491/491 [==============================] - 214s 435ms/step - loss: 0.8877 - accuracy: 0.7031 - val_loss: 0.5676 - val_accuracy: 0.8141\n",
      "Epoch 3/5\n",
      "491/491 [==============================] - 214s 436ms/step - loss: 0.4819 - accuracy: 0.8420 - val_loss: 0.3115 - val_accuracy: 0.8922\n",
      "Epoch 4/5\n",
      "491/491 [==============================] - 214s 436ms/step - loss: 0.3149 - accuracy: 0.8974 - val_loss: 0.2466 - val_accuracy: 0.9219\n",
      "Epoch 5/5\n",
      "491/491 [==============================] - 213s 434ms/step - loss: 0.2209 - accuracy: 0.9291 - val_loss: 0.1892 - val_accuracy: 0.9422\n",
      "Evaluate\n",
      "123/123 [==============================] - 18s 142ms/step - loss: 0.2032 - accuracy: 0.9355\n",
      "Saving model...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\akomm\\AppData\\Local\\Temp\\tmp30k2qag3\\normal_model\\assets\n",
      "Saved model.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from ml_library.model import *\n",
    "from ml_library.config import *\n",
    "\n",
    "\n",
    "# normal_model = get_compiled_model(REIN=False)\n",
    "###################\n",
    "# https://keras.io/guides/sequential_model/ example code\n",
    "#def get_uncompiled_model(REIN=False):\n",
    "if True:\n",
    "    inputs = keras.Input(\n",
    "        shape=(IMG_SIZE, IMG_SIZE, NUM_CHANNELS), name=\"inputs\")\n",
    "    model = keras.Sequential()\n",
    "    # model.add(inputs)\n",
    "    # model.add(resize_and_rescale)\n",
    "\n",
    "    # # REIN Data Augmentation\n",
    "    # if(REIN):\n",
    "    #     model.add(data_augmentation)\n",
    "\n",
    "    model.add(Conv2D(16, 3, padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(64, 3, strides=(3, 3), padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(Conv2D(128, 3, padding='same', activation='gelu'))\n",
    "    # model.add(BatchNormalization)\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # FC Layers w/ Dropout\n",
    "    model.add(Dense(1024, activation='gelu'))\n",
    "    model.add(Dropout(RATE))\n",
    "    model.add(Dense(1024, activation='gelu'))\n",
    "    model.add(Dropout(RATE))\n",
    "\n",
    "    model.add(Dense(NUM_CLASSES, activation=\"softmax\", name=\"predictions\"))\n",
    "\n",
    "    outputs = model.get_layer(name=\"predictions\").output\n",
    "    # build model\n",
    "    model2 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model = model2\n",
    "    #return model\n",
    "\n",
    "\n",
    "#def get_compiled_model(REIN=False):\n",
    "if True:\n",
    "    #model = get_uncompiled_model(REIN)\n",
    "\n",
    "    # model.compile(\n",
    "    #     optimizer=\"rmsprop\",\n",
    "    #     loss=\"sparse_categorical_crossentropy\",\n",
    "    #     metrics=[\"sparse_categorical_accuracy\"],\n",
    "    # )\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                      from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    #return model\n",
    "\n",
    "    normal_model = model\n",
    "##################\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "# hist_norm = normal_model.fit([vae.predict(x_train), y_train], validation_data=val_dataset,\n",
    "#                              validation_steps=10, epochs=NUM_EPOCH, verbose=1)\n",
    "hist_norm = normal_model.fit(train_dataset, validation_data=val_dataset,\n",
    "                             validation_steps=10, epochs=NUM_EPOCH, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluate\")\n",
    "result_normal = normal_model.evaluate(val_dataset)\n",
    "dict(zip(normal_model.metrics_names, result_normal))\n",
    "\n",
    "\n",
    "model_path = os.path.join(model_dir, 'normal_model')\n",
    "normal_model_path = model_path\n",
    "print('Saving model...')\n",
    "normal_model.save(model_path)\n",
    "# tf.saved_model.save(normal_model, model_path)\n",
    "print('Saved model.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train REIN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vae = VariationalAutoEncoder(original_dim, IMG_SIZE*2, IMG_SIZE, REIN=True)\n",
    "# vae.compile(optimizer, loss=mse_loss_fn, metrics=loss_metric)\n",
    "# vae.fit(x_train, x_train, epochs=NUM_EPOCH,  batch_size=64, verbose=1)\n",
    "# vae.evaluate(x_val, x_val, verbose=1)\n",
    "\n",
    "# model_path = os.path.join(model_dir, 'rein_vae')\n",
    "# print('Saving model...')\n",
    "# vae.save(model_path)\n",
    "# tf.saved_model.save(vae, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip REIN model for now\n",
    "if False:\n",
    "    rein_model = get_compiled_model(REIN=True)\n",
    "\n",
    "\n",
    "    # Since the dataset already takes care of batching,\n",
    "    # we don't pass a `batch_size` argument.\n",
    "    # hist_rein = rein_model.fit(vae.predict(train_dataset), validation_data=val_dataset,\n",
    "    #                            validation_steps=10, epochs=NUM_EPOCH, verbose=1)\n",
    "    hist_rein = rein_model.fit(train_dataset, validation_data=val_dataset,\n",
    "                            validation_steps=10, epochs=NUM_EPOCH, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate\")\n",
    "    result_rein = rein_model.evaluate(val_dataset)\n",
    "    dict(zip(rein_model.metrics_names, result_rein))\n",
    "\n",
    "\n",
    "    model_path = os.path.join(model_dir, 'rein_model')\n",
    "    rein_model_path = model_path\n",
    "    print('Saving model...')\n",
    "    rein_model.save(model_path)\n",
    "    # tf.saved_model.save(rein_model, rein_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize(original, augmented):\n",
    "#   fig = plt.figure()\n",
    "#   plt.subplot(1, 2, 1)\n",
    "#   plt.title('Original image')\n",
    "#   plt.imshow(original)\n",
    "\n",
    "#   plt.subplot(1, 2, 2)\n",
    "#   plt.title('Augmented image')\n",
    "#   plt.imshow(augmented)\n",
    "\n",
    "\n",
    "# image, label = next(iter(train_dataset))\n",
    "# _ = plt.imshow(image)\n",
    "# _ = plt.title(get_label_name(label))\n",
    "\n",
    "# flipped = tf.image.flip_left_right(image)\n",
    "# visualize(image, flipped)\n",
    "\n",
    "# grayscaled = tf.image.rgb_to_grayscale(image)\n",
    "# visualize(image, tf.squeeze(grayscaled))\n",
    "# _ = plt.colorbar()\n",
    "\n",
    "\n",
    "# saturated = tf.image.adjust_saturation(image, 3)\n",
    "# visualize(image, saturated)\n",
    "\n",
    "\n",
    "# bright = tf.image.adjust_brightness(image, 0.4)\n",
    "# visualize(image, bright)\n",
    "\n",
    "\n",
    "# cropped = tf.image.central_crop(image, central_fraction=0.5)\n",
    "# visualize(image, cropped)\n",
    "\n",
    "# rotated = tf.image.rot90(image)\n",
    "# visualize(image, rotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test= 12569\n",
      "y_test= 12569\n",
      "<keras.engine.functional.Functional object at 0x0000022EF3ACD550>\n",
      "Predictions\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(32, 32, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29792/3794196080.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predictions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Image Prediction: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;31m# prediction = model.predict(image_x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\akomm\\.conda\\envs\\tf-gpu-38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# print(\"Reg Train Keras version: \", keras.__version__)\n",
    "# print(\"Reg Train TF version: \", tf.__version__)\n",
    "\n",
    "from ml_library.config import *\n",
    "from ml_library.utils import *\n",
    "from ml_library.model import *\n",
    "#from ml_library.w_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# testing_file = './Datasets/GTSRB_Final_Test_Images.zip'\n",
    "testing_file = './Datasets/GTSRB_Online-Test-Images-Sorted.zip'\n",
    "test = generateTensor(testing_file, False)\n",
    "x_test, y_test = test['features'], test['labels']\n",
    "print(\"x_test=\", len(x_test))\n",
    "print(\"y_test=\", len(y_test))\n",
    "#x_test, y_test = preprocess_data(x_test, y_test)\n",
    "#################\n",
    "\n",
    "x = x_test\n",
    "y = y_test\n",
    "if False:\n",
    "#def preprocess_data(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess image data, and convert labels into one-hot\n",
    "\n",
    "    Arguments:\n",
    "        * x: Image data\n",
    "        * y: Labels\n",
    "\n",
    "    Returns:\n",
    "        * Preprocessed x, one-hot version of y\n",
    "    \"\"\"\n",
    "    # Convert from RGB to grayscale if applicable\n",
    "    if GRAYSCALE:\n",
    "        x = rgb_to_gray(x)\n",
    "\n",
    "    # Make all image array values fall within the range -1 to 1\n",
    "    # Note all values in original images are between 0 and 255, as uint8\n",
    "    x = x.astype('float32')\n",
    "    x = (x - 128.) / 128.\n",
    "\n",
    "    # Convert the labels from numerical labels to one-hot encoded labels\n",
    "    y_onehot = np.zeros((y.shape[0], NUM_CLASSES))\n",
    "    for i, onehot_label in enumerate(y_onehot):\n",
    "        onehot_label[y[i]] = 1.\n",
    "    y = y_onehot\n",
    "\n",
    "    x_test = x\n",
    "    y_test = y\n",
    "\n",
    "    #return x, y\n",
    "\n",
    "#################\n",
    "# print(\"x_test shape:\", x_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# normal_model_path = r\"C:\\Users\\akomm\\AppData\\Local\\Temp\\tmp4wcvqiyg\\normal_model\"\n",
    "normal_model_path = r\"C:\\Users\\akomm\\AppData\\Local\\Temp\\tmp30k2qag3\\normal_model\"\n",
    "\n",
    "# # model_path = normal_model_path\n",
    "# model_path = rein_model_path\n",
    "# model = tf.saved_model.load(model_path)\n",
    "# adv_acc = model.evaluate()\n",
    "\n",
    "if True:    # no need of session in TF2\n",
    "#with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    model_path = normal_model_path\n",
    "    # model_path = rein_model_path\n",
    "    #model = tf.saved_model.load(model_path)\n",
    "\n",
    "    # https://githubmemory.com/repo/tensorflow/decision-forests/issues/25\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(model)\n",
    "\n",
    "    # Prepare the validation dataset\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    # test_dataset = test_dataset.batch(64)\n",
    "    \n",
    "    #print(\"Evaluate\")\n",
    "    #acc = model.evaluate(test_dataset)\n",
    "\n",
    "    # image = x_test[np.random.randint(len(x_test))]\n",
    "    image = x_test[:3]\n",
    "\n",
    "    # Show original image for reference\n",
    "    # plt.subplot(3, 3, 1)\n",
    "    plt.imshow(image[1])\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    # for i in range(9):\n",
    "    #     image_x = transform_image(image, ANGLE, TRANSLATION, WARP)\n",
    "    #     plt.subplot(3, 3, i+2)\n",
    "    #     plt.imshow(image_x)\n",
    "    #     plt.title('Transformed Image %d' % (i+1,))\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    #image = x_test[1:2]\n",
    "    \n",
    "    img = image[1]\n",
    "    print(\"Predictions\")\n",
    "    prediction = model.predict(img)\n",
    "    print(\"Image Prediction: \", prediction)\n",
    "    # prediction = model.predict(image_x)\n",
    "    # print(\"Image Prediction: \", prediction)\n",
    "    # print(\"Test Set Accuracy = {:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "    for i in range(20):\n",
    "        rain_images = rain(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"Rain Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = fog(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"fog Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = light(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"light Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" %\n",
    "              i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = dark(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"dark Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = blur(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"Blur deviation %.2d  vs. Accuracy vs Adv_Accuracy：\" %\n",
    "              i, acc, adv_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
