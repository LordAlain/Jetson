{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "# import cupy as cp\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "print(\"Reg Train Keras version: \", keras.__version__)\n",
    "print(\"Reg Train TF version: \", tf.__version__)\n",
    "\n",
    "from ml_library.model import *\n",
    "from ml_library.utils import *\n",
    "from ml_library.config import *\n",
    "from ml_library.w_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_dir = '/models/'\n",
    "tmpdir = tempfile.mkdtemp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importDatasets()\n",
    "\n",
    "\n",
    "# Load data\n",
    "training_file = \"./Datasets/GTSRB_Final_Training_Images.zip\"\n",
    "testing_file = \"./Datasets/GTSRB_Final_Test_Images.zip\"\n",
    "\n",
    "train = generateTensor(training_file)\n",
    "test = generateTensor(testing_file)\n",
    "\n",
    "x_train, y_train = split(train)\n",
    "x_test, y_test = split(test)\n",
    "\n",
    "label_map = map_labels('signnames.csv')\n",
    "\n",
    "#Train/validation split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    np.asarray(x_train), np.asarray(y_train), test_size=VALIDATION_SIZE)\n",
    "\n",
    "\n",
    "valid_ratio = len(x_valid) / len(file_paths)\n",
    "print(\"Train size: {} Validation size: {} ({:0.3f})\").format(\n",
    "    len(x_train),\n",
    "    len(x_valid),\n",
    "    valid_ratio)\n",
    "\n",
    "classes, dist = np.unique(y_train+y_valid, return_counts=True)\n",
    "NUM_CLASSES = len(classes)\n",
    "print(\"No classes:{}\".format(NUM_CLASSES))\n",
    "\n",
    "plt.bar(classes, dist, align='center', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_valid shape:\", x_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(original, augmented):\n",
    "  fig = plt.figure()\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title('Original image')\n",
    "  plt.imshow(original)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.title('Augmented image')\n",
    "  plt.imshow(augmented)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_ds))\n",
    "_ = plt.imshow(image)\n",
    "_ = plt.title(get_label_name(label))\n",
    "\n",
    "flipped = tf.image.flip_left_right(image)\n",
    "visualize(image, flipped)\n",
    "\n",
    "grayscaled = tf.image.rgb_to_grayscale(image)\n",
    "visualize(image, tf.squeeze(grayscaled))\n",
    "_ = plt.colorbar()\n",
    "\n",
    "\n",
    "saturated = tf.image.adjust_saturation(image, 3)\n",
    "visualize(image, saturated)\n",
    "\n",
    "\n",
    "bright = tf.image.adjust_brightness(image, 0.4)\n",
    "visualize(image, bright)\n",
    "\n",
    "\n",
    "cropped = tf.image.central_crop(image, central_fraction=0.5)\n",
    "visualize(image,cropped)\n",
    "\n",
    "rotated = tf.image.rot90(image)\n",
    "visualize(image, rotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = IMG_SIZE*IMG_SIZE\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "# Now we get a test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(\n",
    "    1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "vae = VariationalAutoEncoder(original_dim, IMG_SIZE*2, IMG_SIZE, REIN=False)\n",
    "vae.compile(optimizer, loss= mse_loss_fn, metrics= loss_metric)\n",
    "vae.fit(x_train, x_train, epochs=NUM_EPOCH,  batch_size=64, verbose=1)\n",
    "vae.evaluate(x_valid, x_valid, verbose=1)\n",
    "\n",
    "\n",
    "# epochs = NUM_EPOCH\n",
    "\n",
    "# # Iterate over epochs.\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     for step, x_batch_train in enumerate(train_dataset):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             reconstructed = vae(x_batch_train)\n",
    "#             # Compute reconstruction loss\n",
    "#             loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "#             loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "#         grads = tape.gradient(loss, vae.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "#         loss_metric(loss)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))\n",
    "\n",
    "\n",
    "model_path = os.path.join(tmpdir, 'normal_vae')\n",
    "module(tf.constant(0.))\n",
    "print('Saving model...')\n",
    "vae.save(model_path)\n",
    "# tf.saved_model.save(vae, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normal_model = get_compiled_model(REIN=False)\n",
    "\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "hist_norm = normal_model.fit(vae.predict(train_dataset), validation_data=val_dataset,\n",
    "                 validation_steps=10, epochs=NUM_EPOCH, verbose= 1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluate\")\n",
    "result_normal = normal_model.evaluate(test_dataset)\n",
    "dict(zip(normal_model.metrics_names, result_normal))\n",
    "\n",
    "\n",
    "model_path = os.path.join(tmpdir, 'normal_model')\n",
    "module(tf.constant(0.))\n",
    "print('Saving model...')\n",
    "normal_model.save(model_path)\n",
    "# tf.saved_model.save(normal_model, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train REIN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae = VariationalAutoEncoder(original_dim, IMG_SIZE*2, IMG_SIZE, REIN=True)\n",
    "vae.compile(optimizer, loss=mse_loss_fn, metrics=loss_metric)\n",
    "vae.fit(x_train, x_train, epochs=NUM_EPOCH,  batch_size=64, verbose=1)\n",
    "vae.evaluate(x_valid, x_valid, verbose=1)\n",
    "\n",
    "model_path = os.path.join(tmpdir, 'rein_vae')\n",
    "module(tf.constant(0.))\n",
    "print('Saving model...')\n",
    "vae.save(model_path)\n",
    "# tf.saved_model.save(vae, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rein_model = get_compiled_model(REIN=True)\n",
    "\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "hist_rein = rein_model.fit(vae.predict(train_dataset), validation_data=val_dataset,\n",
    "                      validation_steps=10, epochs=NUM_EPOCH, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluate\")\n",
    "result_rein = rein_model.evaluate(test_dataset)\n",
    "dict(zip(rein_model.metrics_names, result_rein))\n",
    "\n",
    "\n",
    "model_path = os.path.join(tmpdir, 'rein_model')\n",
    "module(tf.constant(0.))\n",
    "print('Saving model...')\n",
    "rein_model.save(model_path)\n",
    "# tf.saved_model.save(rein_model, rein_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing_file = './Datasets/GTSRB_Final_Test_Images.zip'\n",
    "test = generateTensor(testing_file)\n",
    "X_test, y_test = preprocess_data(test)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# # model_path = normal_model_path\n",
    "# model_path = rein_model_path\n",
    "# model = tf.saved_model.load(model_path)\n",
    "# adv_acc = model.evaluate()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # model_path = normal_model_path\n",
    "    model_path = rein_model_path\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    acc = model.evaluate(x_test, y_test)\n",
    "    print(\"Test Set Accuracy = {:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "    for i in range(20):\n",
    "        rain_images = rain(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"Rain Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = fog(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"fog Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = light(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"light Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" %\n",
    "              i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = dark(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"dark Drops %.2d  vs. Accuracy vs Adv_Accuracy：\" % i, acc, adv_acc)\n",
    "    \n",
    "    for i in range(20):\n",
    "        rain_images = blur(x_test, i)\n",
    "        adv_acc = model.evaluate(rain_images, y_test)\n",
    "        print(\"Blur deviation %.2d  vs. Accuracy vs Adv_Accuracy：\" %\n",
    "              i, acc, adv_acc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
